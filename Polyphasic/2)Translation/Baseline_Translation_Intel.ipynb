{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYkH5y3Qzckz"
      },
      "source": [
        "\n",
        "\n",
        "# Translation Models\n",
        "\n",
        "\n",
        "Machine translation is a pivotal field within natural language processing (NLP) that focuses on automating the conversion of text or speech from one language to another. It relies on sophisticated models and techniques to accomplish this challenging task effectively. One of the cornerstone methods in machine translation is the sequence-to-sequence (seq2seq) model, which employs deep neural networks to encode input text and then decode it into the target language. This technique has revolutionized translation tasks by learning to capture complex linguistic nuances and contextual information. Additionally, other models like Transformer-based models, including the famous BERT and GPT-3, have also made significant strides in translation, leveraging attention mechanisms to excel in various language pairs and domains. The choice of model depends on specific translation requirements, language pairs, and the quality of available training data. In this Colab file, we havee given a basic demo on how tto use the dataset and work on a simple seq2seq moel usig RNN.Your task will be to improve the model to the maximum you can ,make prediction on the test dataset given and write a code to generate the BLEU score of you prediction compared to original.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nS1d9rgev8J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbtD0ux5ew7d"
      },
      "outputs": [],
      "source": [
        "##Loading and processing data\n",
        "eng_fr = pd.read_csv(\"nlp_intel_train.csv\")\n",
        "eng_fr_test = pd.read_csv(\"nlp_intel_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs9CABoPfty1"
      },
      "outputs": [],
      "source": [
        "eng_fr = eng_fr.dropna(axis=0, how=\"any\", subset=None, inplace=False)\n",
        "eng_fr_test = eng_fr_test.dropna(axis=0, how=\"any\", subset=None, inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cenL2y2df2p1"
      },
      "outputs": [],
      "source": [
        "##Tokenizer and padding\n",
        "\n",
        "def tokenize(data):\n",
        "  t = Tokenizer()\n",
        "  t.fit_on_texts(data)\n",
        "  return t\n",
        "def training_sequences(tokenizer, m_length, data):\n",
        "    seq = tokenizer.texts_to_sequences(data)\n",
        "    seq = pad_sequences(seq, maxlen = m_length, padding='post')\n",
        "    return seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKK20PUMg4K3"
      },
      "outputs": [],
      "source": [
        "#Preprocessing by tokenization and padding\n",
        "#return processed data and tokenizer\n",
        "def preprocess(x, y):\n",
        "\n",
        "    x_tk = tokenize(x)\n",
        "    y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = training_sequences(x_tk,55,x)\n",
        "    preprocess_y = training_sequences(y_tk,55,y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWl-vgZziSeD"
      },
      "outputs": [],
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(eng_fr[\"en\"].tolist(), eng_fr[\"fr\"].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yU5F-7eii6N",
        "outputId": "ad70ba11-7a86-4e26-9d77-0c7f85b3ffda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max English sentence length: 55\n",
            "Max French sentence length: 55\n",
            "English vocabulary size: 21789\n",
            "French vocabulary size: 27712\n"
          ]
        }
      ],
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP1THRqril3N"
      },
      "outputs": [],
      "source": [
        "#Final output funtion\n",
        "def logits_to_text(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ' '\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoy5079ni0z3"
      },
      "outputs": [],
      "source": [
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(french_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(GRU(256, return_sequences=True))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dense(english_vocab_size, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bRXEU8TjWNL",
        "outputId": "2f87dfe0-52c2-447d-f16d-ad3802a180db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preproc_french_sentences.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu1W91SAi1mv",
        "outputId": "1d6c6309-658a-4095-d073-61f47f8a8309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 55, 256)           7094528   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 55, 256)           394752    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 55, 1024)          263168    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 55, 21790)         22334750  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30087198 (114.77 MB)\n",
            "Trainable params: 30087198 (114.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "225/225 [==============================] - 67s 275ms/step - loss: 3.0880 - accuracy: 0.6692 - val_loss: 2.4539 - val_accuracy: 0.6950\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b00fefe2ce0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp_x =pad_sequences(preproc_french_sentences, maxlen = 55, padding = 'post')\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "# Train\n",
        "model = bd_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_english_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(tmp_x, preproc_english_sentences, batch_size=64, epochs=1, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdY7DBVQksvf",
        "outputId": "2b083e54-4d42-4a07-aa8c-25ab45b55073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction:\n",
            "1/1 [==============================] - 0s 409ms/step\n",
            "the the the the the the the the the the                                                                                          \n",
            "\n",
            "Correct Translation:\n",
            "The club was very active and they twice organized the annual conference of the Amateur Astronomy Federation of Quebec in 1990 and 1997.\n",
            "\n",
            "Original text:\n",
            "Le club est très actif et organise à deux occasions (en 1990 et 1997) le congrès annuel de la Fédération des Astronomes Amateurs du Québec.\n"
          ]
        }
      ],
      "source": [
        "i= 1\n",
        "\n",
        "\n",
        "print(\"Prediction:\")\n",
        "print(logits_to_text(model.predict(tmp_x[[i]])[0], english_tokenizer))\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(eng_fr[\"en\"].tolist()[i])\n",
        "print(\"\\nOriginal text:\")\n",
        "print(eng_fr[\"fr\"].tolist()[i])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
